{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformers and Hugging Face"
      ],
      "metadata": {
        "id": "15nJjjJdGKAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are transformers in NLP?\n",
        "\n",
        "Transformers is the new simple yet powerful neural network architecture introduced by Google Brain in 2017 with their famous research paper “Attention is all you need.” It is based on the attention mechanism instead of the sequential computation as we might observe in recurrent networks.\n",
        "\n",
        "What are the main components of transformers?\n",
        "\n",
        "Similar to recurrent networks, transformers also have two main blocs: encoder and decoder, each one having a self-attention mechanism. The first version of transformers had RNN and LSTM encoder-decoder architecture, which have been changed later into self-attention and feed-forward networks.\n",
        "\n",
        "The following section provides a general overview of the main components of each block of transformers."
      ],
      "metadata": {
        "id": "1ZpX2KO8GLAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input sentence preprocessing stage\n",
        "\n",
        "This section contains two main steps: (1) the generation of the embeddings of the input sentence, and (2) the computation of the positional vector of each word in the input sentence. All the computations are performed the same way for both the source sentence (before the encoder block) and the target sentence (before the decoder block).\n",
        "\n",
        "Embedding of the input data\n",
        "\n",
        "Before generating the embeddings of the input data, we start by performing the tokenization, then create the embedding of each individual word without paying attention to their relationship in the sentence.\n",
        "\n",
        "Positional encoding\n",
        "\n",
        "The tokenization task discards any notion of relations that existed in the input sentence. The positional encoding tries to create the original cyclic nature by generating a context vector for each word.\n",
        "\n",
        "Encoder bloc\n",
        "\n",
        "At the end of the previous step, we get for each word two vectors: (1) the embedding and (2) its context vector. These vectors are added to create a single vector for each word, which is then transmitted to the encoder.\n",
        "\n",
        "Multi-head attention\n",
        "\n",
        "As mentioned previously, we lost all notion of a relationship. The goal of the attention layer is to capture the contextual relationships existing between different words in the input sentence. This step ends up generating an attention vector for each word.\n",
        "\n",
        "Position-wise feed-forward net (FFN)\n",
        "\n",
        "At this stage, a feed-forward neural network is applied to every attention vector to transform them into a format that is expected by the next multi-head attention layer in the decoder.\n",
        "\n",
        "Decoder block\n",
        "\n",
        "The decoder block consists of three main layers: masked multi-head attention, multi-head attention, and a position-wise feed-forward network. We already understand the last two layers, which are the same in the encoder.\n",
        "\n",
        "The decoder comes into the equation during the training of the network, and it receives two main inputs: (1) the attention vectors of the input sentence we want to translate and (2) the translated target sentences in English.\n",
        "\n",
        "So, what is the masked multi-head attention layer responsible for?\n",
        "\n",
        "During the generation of the next English word, the network is allowed to use all the words from the French word. However, when dealing with a given word in the target sequence (English translation), the network only has to access the previous words because making the next ones available will lead the network to “cheat” and not make any effort to learn properly. Here is where the masked multi-head attention layer has all its benefits. It masks those next words by transforming them into zeros so that they can’t be used by the attention network.\n",
        "\n",
        "The result of the masked multi-head attention layer passes through the rest of the layers in order to predict the next word by generating a probability score.\n",
        "\n",
        "This architecture was successful because of the following reasons:\n",
        "\n",
        "The total computational complexity at each layer is lower compared to RNNs.\n",
        "It totally got rid of all necessity of recurrence and allows sequence parallelization, unlike RNNs that expect the input to be in sequence.\n",
        "RNNs are not efficient with learning from long-range sequences because of the lengths of the path forward and backward signals in the network. This path is shortened using self-attention, which improves the learning process.\n",
        "Transfer Learning in NLP\n",
        "\n",
        "Training deep neural networks such as transformers from scratch is not an easy task, and might present the following challenges:\n",
        "\n",
        "Finding the required amount of data for the target problem can be time-consuming\n",
        "Getting the necessary computation resources like GPUs to train such deep networks can be very costly.  \n",
        "Using transfer learning can have many benefits, such as reducing the training time, speeding up the training process of new models, and decreasing project delivery time.\n",
        "\n",
        "Imagine building a model from scratch to translate Mandingo language into Wolof, which are both low resources languages. Gathering data related to those languages is costly. Instead of going through all these challenges, one can re-use pre-trained deep-neural networks as the starting point for training the new model.\n",
        "\n",
        "Such models have been trained on a huge corpus of data, made available by someone else (moral person, organization, etc.), and evaluated to work very well on language translation tasks such as French to English.\n",
        "\n",
        "If you are new to NLP, this Introduction to Natural Language Processing in Python course can provide you with the fundamental skills to perform and solve real-world problems.\n",
        "\n",
        "But what do you mean by re-use of deep-neural networks?\n",
        "\n",
        "The re-use of the model involves choosing the pre-trained model that is similar to your use case, refining the input-output pair data of your target task, and retraining the head of the pre-trained model by using your data.\n",
        "\n",
        "The introduction of Transformers has led to the development of state-of-the-art transfer learning models such as:\n",
        "\n",
        "BERT, short for Bidirectional Encoder Representations from Transformers, was developed by Google researchers in 2018. It helps to solve the most common language tasks such as named entity recognition, sentiment analysis, question-answering, text-summarization, etc. Read more about BERT in this NLP tutorial.\n",
        "GPT3 (Generative Pre-Training-3), proposed by OpenAI researchers. It is a multi-layer transformer, mainly used to generate any type of text. GPT models are capable of producing human-like text responses to a given question. This amazing article can provide you with deeper information about what makes GPT3 unique, the technology powering it, its risks, and its limitations.\n",
        "An introduction to Hugging Face Transformers\n",
        "\n",
        "Hugging Face is an AI community and Machine Learning platform created in 2016 by Julien Chaumond, Clément Delangue, and Thomas Wolf. It aims to democratize NLP by providing Data Scientists, AI practitioners, and Engineers immediate access to over 20,000 pre-trained models based on the state-of-the-art transformer architecture. These models can be applied to:\n",
        "\n",
        "Text in over 100 languages for performing tasks such as classification, information extraction, question answering, generation, generation, and translation.\n",
        "Speech, for tasks such as object audio classification and speech recognition.\n",
        "Vision for object detection, image classification, segmentation.\n",
        "Tabular data for regression and classification problems.\n",
        "Reinforcement Learning transformers.\n",
        "Hugging Face Transformers also provides almost 2000 data sets and layered APIs, allowing programmers to easily interact with those models using almost 31 libraries. Most of them are deep learning, such as  Pytorch, Tensorflow, Jax, ONNX, Fastai, Stable-Baseline 3, etc.\n",
        "\n",
        "These courses are a great introduction to using Pytorch and Tensorflow for respectively building deep convolutional neural networks. Other components of the Hugging Face Transformers are the Pipelines.\n",
        "\n",
        "What are Pipelines in Transformers?\n",
        "\n",
        "They provide an easy-to-use API through pipeline() method for performing inference over a variety of tasks.\n",
        "They are used to encapsulate the overall process of every Natural Language Processing task, such as text cleaning, tokenization, embedding, etc.\n",
        "The pipeline() method has the following structure:"
      ],
      "metadata": {
        "id": "JKQ6g0ESGQVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# To use a default model & tokenizer for a given task(e.g. question-answering)\n",
        "pipeline(\"<task-name>\")\n",
        "\n",
        "# To use an existing model\n",
        "pipeline(\"<task-name>\", model=\"<model_name>\")\n",
        "\n",
        "# To use a custom model/tokenizer\n",
        "pipeline('<task-name>', model='<model name>',tokenizer='<tokenizer_name>')"
      ],
      "metadata": {
        "id": "g4TTkdIFGWLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load the data from the path\n",
        "data_path = \"datacamp_workspace_export_2022-08-08 07_56_40.csv\"\n",
        "news_data = pd.read_csv(data_path, error_bad_lines=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Show data information\n",
        "news_data.info()"
      ],
      "metadata": {
        "id": "VLB_i9vGGXlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers sentencepiece\n",
        "from transformers import MarianTokenizer, MarianMTModel"
      ],
      "metadata": {
        "id": "pXCfRNxtGaiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the name of the model\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
        "\n",
        "# Get the tokenizer\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "# Instantiate the model\n",
        "model = MarianMTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "IUTqMdF6Gclt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_batch_texts(language_code, batch_texts):\n",
        "    formated_bach = [\">>{}<< {}\".format(language_code, text) for text in\n",
        "\n",
        "                batch_texts]\n",
        "return formated_bach"
      ],
      "metadata": {
        "id": "KdFiXIRiGeHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n",
        "\n",
        "  # Prepare the text data into appropriate format for the model\n",
        "  formated_batch_texts = format_batch_texts(language, batch_texts)\n",
        "\n",
        "  # Generate translation using model\n",
        "  translated = model.generate(**tokenizer(formated_batch_texts,\n",
        "\n",
        "                                          return_tensors=\"pt\", padding=True))\n",
        "\n",
        "  # Convert the generated tokens indices back into text\n",
        "  translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "  return translated_texts"
      ],
      "metadata": {
        "id": "Z-Qu3IxhGfpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model translation from the original language (English) to French\n",
        "translated_texts = perform_translation(english_texts, trans_model, trans_model_tkn)\n",
        "\n",
        "# Create wrapper to properly format the text\n",
        "from textwrap import TextWrapper\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = TextWrapper(width=80)\n",
        "\n",
        "for text in translated_texts:\n",
        "  print(\"Original text: \\n\", text)\n",
        "  print(\"Translation : \\n\", text)\n",
        "  print(print(wrapper.fill(text)))\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "Zxl6xDCXGg9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZERO-SHOT CLASSIFICATION"
      ],
      "metadata": {
        "id": "3T19NKnkGmQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "q7o98M27GqEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_labels = [\"tech\", \"politics\", \"business\", \"finance\"]"
      ],
      "metadata": {
        "id": "PBNC_M3BGq9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_classifier = pipeline(\"zero-shot-classification\",\n",
        "\n",
        "                           model='joeddav/xlm-roberta-large-xnli')"
      ],
      "metadata": {
        "id": "xTK1l6VqGtz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the first description\n",
        "prediction = my_classifier(english_texts[0], candidate_labels, multi_class = True)\n",
        "pd.DataFrame(prediction).drop([\"sequence\"], axis=1)"
      ],
      "metadata": {
        "id": "nM6FJBxjGvPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For the last description\n",
        "prediction = my_classifier(english_texts[-1], candidate_labels, multi_class = True)\n",
        "pd.DataFrame(prediction).drop([\"sequence\"], axis=1)"
      ],
      "metadata": {
        "id": "He5G8rl6GxDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "KNyHse_qG26o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "distil_bert_model = pipeline(task=\"sentiment-analysis\", model=model_checkpoint)"
      ],
      "metadata": {
        "id": "Mymi7Mj3G49K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the predictions\n",
        "distil_bert_model(english_texts[1:])"
      ],
      "metadata": {
        "id": "cetuQEQMG8X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer"
      ],
      "metadata": {
        "id": "F9GgaNQOG9gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "task = 'question-answering'\n",
        "QA_model = pipeline(task, model=model_checkpoint, tokenizer=model_checkpoint)"
      ],
      "metadata": {
        "id": "HP6QVA_2G_ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA_input = {\n",
        "          'question': 'when is Apple hosting an event?',\n",
        "          'context': english_texts[-1]\n",
        "          }"
      ],
      "metadata": {
        "id": "9VBLQ4Q8HB72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_response = QA_model(QA_input)\n",
        "pd.DataFrame([model_response])"
      ],
      "metadata": {
        "id": "51vx0jZ1HFkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}